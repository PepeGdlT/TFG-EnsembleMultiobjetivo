{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "338b497d868a974a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9e2052865e41f0ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Requerimientos",
   "id": "1c4c00c13597af59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import joblib\n",
    "\n",
    "from pandas.core.common import random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import arff\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")"
   ],
   "id": "44d75185744bd45d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Lectura de los datos\n",
    "#### Input:\n",
    "  - $file\\_path$: Nombre completo con path de la base de datos .arff a cargar\n",
    "\n",
    "#### Output:\n",
    "  - $X$: Atributos de entrada num√©ricos\n",
    "  - $y$: Salida num√©rica"
   ],
   "id": "3a814ceffefb2685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "# CORRECCIONES IMPORTANTES para wrapped\n",
    "# cambiar x y a validacion y test -> evaluate de train y validacion para q sea comparable con el voting√ß#\n",
    "# separar x  y en training y validacion\n",
    "# en evaluate en vez de cv -> x_train y x_val y lo mismo para y\n",
    "\n",
    "\n",
    "\n",
    "def LoadData(file_path):\n",
    "    # 1. Cargar con liac-arff\n",
    "    with open(file_path, 'r') as f:\n",
    "        dataset = arff.load(f)\n",
    "\n",
    "    col_names = [attr[0] for attr in dataset['attributes']]\n",
    "    df = pd.DataFrame(dataset['data'], columns=col_names)\n",
    "\n",
    "    # 2. Limpieza b√°sica\n",
    "    df.replace([None], np.nan, inplace=True)\n",
    "    filename = file_path.lower()\n",
    "\n",
    "    # --- CORRECCIONES ESPEC√çFICAS ---\n",
    "\n",
    "    # CASO US CRIME: Eliminar ID in√∫til\n",
    "    if 'crime' in filename and 'communityname' in df.columns:\n",
    "        df = df.drop(columns=['communityname'])\n",
    "\n",
    "    # CASO BOSTON: Arreglar columnas que cargan mal\n",
    "    if 'boston' in filename:\n",
    "        cols_to_fix = ['CHAS', 'RAD']\n",
    "        for col in cols_to_fix:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 3. FORZADO NUM√âRICO (SOLO REGRESI√ìN)\n",
    "    # Convertimos todo a n√∫meros.\n",
    "    # En Abalone, 'Sex' se convertir√° enteramente en NaNs.\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 4. BORRAR COLUMNAS VAC√çAS (EL FIX PARA EL ERROR)\n",
    "    # Si una columna es 100% NaN (como 'Sex' en Abalone), la borramos YA.\n",
    "    # As√≠ el imputer no se queja y los tama√±os coinciden.\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # 5. Imputaci√≥n de nulos restantes\n",
    "    # Si quedan huecos sueltos, los rellenamos con la media\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        # Usamos un DataFrame nuevo para evitar conflictos de √≠ndices\n",
    "        data_imputed = imputer.fit_transform(df)\n",
    "        df = pd.DataFrame(data_imputed, columns=df.columns)\n",
    "\n",
    "    # 6. Separar X e y\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    return X, y"
   ],
   "id": "4b4470680c49a9c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Par√°metros del algoritmo evolutivo\n",
    "- $G$: N√∫mero de generaciones\n",
    "- $N$: Tama√±o de la poblaci√≥n\n",
    "- $p\\_c$: Probabilidad de cruce\n",
    "- $p\\_m$: Probabilidad de mutaci√≥n\n",
    "- $random\\_state$: Semilla para reproducibilidad\n",
    "- $X,y$: Datos\n",
    "- $Phi$: Algoritmos de aprendizaje"
   ],
   "id": "76f5eec4528e7ca7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, KBinsDiscretizer\n",
    "\n",
    "# --- 1. PAR√ÅMETROS GEN√âTICOS ---\n",
    "# G pasa a ser el techo real de generaciones; el EA har√° early-stopping antes si converge.\n",
    "MAX_GEN = 150\n",
    "PACIENCIA = 25\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "G = MAX_GEN\n",
    "N = 50\n",
    "p_c = 0.85\n",
    "p_m = 0.2\n",
    "\n",
    "# --- 2. RUTAS ---\n",
    "DATA_DIR = \"../data/regression\"\n",
    "RESULTS_DIR = \"../results/regression\"\n",
    "MODELOS_DIR = \"../modelos_ajustados\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    'Boston':    os.path.join(DATA_DIR, 'boston.arff'),\n",
    "    'Concrete':  os.path.join(DATA_DIR, 'concrete.arff'),\n",
    "    'US_Crime':  os.path.join(DATA_DIR, 'us_crime.arff'),\n",
    "    'Abalone':   os.path.join(DATA_DIR, 'abalone.arff'),\n",
    "    'Elevators': os.path.join(DATA_DIR, 'elevators.arff'),\n",
    "}\n",
    "\n",
    "# Nombres de los modelos base (en el mismo orden en que fueron guardados por 00_1_ajuste_modelos)\n",
    "model_names = [\"ET\", \"RIDGE\", \"KNN\"]\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada.\")\n",
    "print(f\"   Modelos ajustados en: {os.path.abspath(MODELOS_DIR)}\")\n",
    "print(f\"   Resultados ir√°n a:    {os.path.abspath(RESULTS_DIR)}\")"
   ],
   "id": "4220585b180308d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definici√≥n del problema de optimizaci√≥n\n",
    "\n",
    "$\\textit{Minimizar} \\ f(\\textbf{x}) = RMSE(\\textbf{x},D_{train},D_{val},\\Phi)$\n",
    "- $\\textbf{x}=(\\textbf{matriz})$ es el vector de variables de decisi√≥n, donde:\n",
    "  - $matriz_{i,j}\\in\\{0,1\\}$ indica si para el algoritmos de aprendizaje $i$, el atributos $j$ se ha seleccionado.\n",
    "  - $vector_i\\in\\{0,1\\}$ indica si el algoritmo de aprendizaje $i$ se ha seleccionado\n",
    "- $D_{train}=(X_{train},y_{train})$: Conjunto de datos de entrenamiento\n",
    "- $D_{val}=(X_{val},y_{val})$: Conjunto de datos de validaci√≥n\n",
    "- $\\Phi$: Algoritmos de aprendizaje\n"
   ],
   "id": "2e1e5abc9997b0a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, Phi):\n",
    "\n",
    "        # ------- DATOS --------\n",
    "        self.X_train = np.asarray(X_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.X_val   = np.asarray(X_val)\n",
    "        self.y_val   = np.asarray(y_val)\n",
    "\n",
    "        # ------- MODELOS --------\n",
    "        self.Phi = Phi\n",
    "\n",
    "        # ------- DIMENSIONES -------\n",
    "        self.n = self.X_train.shape[1]     # n¬∫ de atributos\n",
    "        self.m = len(self.Phi)             # n¬∫ modelos base\n",
    "\n",
    "        # ------- BUFFERS REUTILIZABLES (Optimizaci√≥n de memoria) -------\n",
    "        self.X_train_selected = np.empty((self.X_train.shape[0], self.n), dtype=self.X_train.dtype)\n",
    "        self.X_val_selected   = np.empty((self.X_val.shape[0],   self.n), dtype=self.X_val.dtype)\n",
    "        self.y_val_pred       = np.empty(self.y_val.shape[0], dtype=self.y_val.dtype)\n",
    "\n",
    "    # =====================================================================================\n",
    "    #                               FUNCI√ìN DE EVALUACI√ìN (Fitness)\n",
    "    # =====================================================================================\n",
    "    def f(self, matriz, vector):\n",
    "        \"\"\"\n",
    "        Calcula el RMSE del ensemble seleccionado por el individuo.\n",
    "        \"\"\"\n",
    "\n",
    "        # √≠ndices de modelos activos\n",
    "        vector_index = np.flatnonzero(vector)\n",
    "        n_selected = len(vector_index)\n",
    "\n",
    "        # 1. PROTECCI√ìN: Si no hay modelos seleccionados, devolvemos error infinito\n",
    "        if n_selected == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        self.y_val_pred[:] = 0\n",
    "\n",
    "        for k, index in enumerate(vector_index):\n",
    "            # Selecci√≥n de columnas (Features)\n",
    "            cols_selected = np.flatnonzero(matriz[:, index])\n",
    "            p = len(cols_selected)\n",
    "\n",
    "            # Si el modelo no tiene features seleccionadas, usamos todas (Fallback)\n",
    "            if p == 0:\n",
    "                cols_selected = np.arange(self.n)\n",
    "                p = self.n\n",
    "\n",
    "            # Copia r√°pida de datos usando buffers\n",
    "            X_train_selected = self.X_train_selected[:, :p]\n",
    "            np.take(self.X_train, cols_selected, axis=1, out=X_train_selected)\n",
    "\n",
    "            X_val_selected = self.X_val_selected[:, :p]\n",
    "            np.take(self.X_val, cols_selected, axis=1, out=X_val_selected)\n",
    "\n",
    "            # 2. IMPORTANTE: Clonar modelo para entrenar desde cero\n",
    "            model = clone(self.Phi[index])\n",
    "            model.fit(X_train_selected, self.y_train)\n",
    "            self.y_val_pred[:] += model.predict(X_val_selected)\n",
    "\n",
    "        # Promedio de las predicciones (Voting/Stacking simple)\n",
    "        self.y_val_pred[:] /= n_selected\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, self.y_val_pred))\n",
    "\n",
    "        return rmse"
   ],
   "id": "3e7b650bd9ae8152",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definici√≥n de individuo",
   "id": "6afdd4a575f4a430"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Individuo:\n",
    "    def __init__(self,problem):\n",
    "        self.matriz = np.zeros((problem.n, problem.m), dtype=int) # selecci√≥n de atributos para cada algoritmo de aprendizaje\n",
    "        self.vector = np.zeros(problem.m, dtype=int)  # selecci√≥n de algoritmos de aprendizaje\n",
    "        self.f = 0.0\n",
    "    def __lt__(self, ind):\n",
    "        return self.f < ind.f"
   ],
   "id": "832a794a5fd9c5fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inicializaci√≥n de la poblaci√≥n",
   "id": "c97ecb0acae6d0e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def InitializePopulation(P):\n",
    "    for I in P:\n",
    "        I.matriz[:] = np.random.randint(0, 2, size=I.matriz.shape)\n",
    "        I.vector[:] = np.random.randint(0 ,2, size=I.vector.size)"
   ],
   "id": "788679e1a1477a1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funci√≥n de reparo",
   "id": "1489c2b0dd92f64d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def repair(I):\n",
    "\n",
    "    n = I.matriz.shape[0]\n",
    "    m = I.matriz.shape[1]\n",
    "\n",
    "    n_selected = I.vector.sum()\n",
    "\n",
    "    # --- 1) garantizar >= 2 modelos base ---\n",
    "    if n_selected == 0:\n",
    "        idx = np.random.choice(m, 2, replace=False)\n",
    "        I.vector[:] = 0\n",
    "        I.vector[idx] = 1\n",
    "\n",
    "    elif n_selected == 1:\n",
    "        selected_idx = np.flatnonzero(I.vector)[0]\n",
    "        rem = np.delete(np.arange(m), selected_idx)\n",
    "        I.vector[np.random.choice(rem)] = 1\n",
    "\n",
    "    # √≠ndices de modelos base activos\n",
    "    Phi_index = np.flatnonzero(I.vector)\n",
    "\n",
    "    # --- 2) garantizar ‚â•1 atributo por base seleccionada ---\n",
    "    cols_empty = Phi_index[np.sum(I.matriz[:, Phi_index], axis=0) == 0]\n",
    "    if len(cols_empty) > 0:\n",
    "        rand_rows = np.random.randint(0, n, size=len(cols_empty))\n",
    "        I.matriz[rand_rows, cols_empty] = 1  # in-place\n"
   ],
   "id": "ea2c2ce597b69e52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funci√≥n de evaluaci√≥n",
   "id": "a71c1821433de1c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate(I,problem):\n",
    "    I.f = problem.f(I.matriz,I.vector)"
   ],
   "id": "6f882c13b38b56d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Copia de individuo",
   "id": "688caf41634d5522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def copia(I1, I2):\n",
    "    I1.matriz[:] = I2.matriz\n",
    "    I1.vector[:] = I2.vector\n",
    "    I1.f = I2.f"
   ],
   "id": "2020567b68c5e5a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecci√≥n por torneo binario",
   "id": "92ee741d1d4cae4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def binary_tournament_selection(P):\n",
    "    return min(np.random.choice(P,2,replace=False))"
   ],
   "id": "6464ed9235b540a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cruce\n",
    "- Cruce uniforme con probabilidad $p\\_c$"
   ],
   "id": "d8e977b3584f01cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def crossover(I1,I2,p_c):\n",
    "    if np.random.random()<=p_c:\n",
    "        for l in range(I1.vector.size):\n",
    "            if np.random.random()<=0.5:\n",
    "                I1.vector[l], I2.vector[l] = I2.vector[l], I1.vector[l]\n",
    "        for l1 in range(I1.matriz.shape[0]):\n",
    "            for l2 in range(I1.matriz.shape[1]):\n",
    "                if np.random.random()<=0.5:\n",
    "                    I1.matriz[l1][l2], I2.matriz[l1][l2] = I2.matriz[l1][l2], I1.matriz[l1][l2]"
   ],
   "id": "2f41df124908fd8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mutaci√≥n\n",
    "- Mutaci√≥n din√°mica (B√§ck): $p_m = 1/L$ donde $L$ es el tama√±o total del cromosoma\n",
    "- Si se pasa un valor fijo de $p\\_m$ se usa ese; si es `None` se aplica la regla de B√§ck"
   ],
   "id": "f5200bf22f17ae1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def mutation(I, p_m=None):\n",
    "    # --- Regla de B√§ck: mutaci√≥n din√°mica ---\n",
    "    # L = total de genes del cromosoma (bits del vector + bits de la matriz)\n",
    "    # p_m = 1/L garantiza que, en media, se mute exactamente 1 gen por individuo,\n",
    "    # independientemente del tama√±o del problema (clave para datasets de 126 features).\n",
    "    if p_m is None:\n",
    "        L = I.vector.size + I.matriz.size\n",
    "        p_m = 1.0 / max(1, L)\n",
    "\n",
    "    for l in range(I.vector.size):\n",
    "        if np.random.random() <= p_m:\n",
    "            I.vector[l] = np.random.randint(2)\n",
    "    for l1 in range(I.matriz.shape[0]):\n",
    "        for l2 in range(I.matriz.shape[1]):\n",
    "            if np.random.random() <= p_m:\n",
    "                I.matriz[l1][l2] = np.random.randint(2)"
   ],
   "id": "f238409df7c72602",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Progreso del algoritmo",
   "id": "31457df02fa5e616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def IniciaProgreso(best_individuo,G):\n",
    "    best_outputs = [0]*(G+1) # Para visualizar la gr√°fica de evoluci√≥n al final\n",
    "    progression_bar = tqdm(total=G, leave=False)\n",
    "    ActualizaProgreso(best_individuo,0,best_outputs,progression_bar)\n",
    "    return best_outputs,progression_bar\n",
    "\n",
    "def ActualizaProgreso(best_individuo,gen,best_outputs,progression_bar):\n",
    "    best_fitness = best_individuo.f\n",
    "    best_rmse = best_fitness\n",
    "    progression_bar.set_description(\"Generation: %s RMSE: %s \" % (str(gen), str(best_rmse)))\n",
    "    best_outputs[gen] = best_fitness # A√±adir mejor fitness (para visualizaci√≥n)\n",
    "    progression_bar.update(1)"
   ],
   "id": "71b385b46ba907f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Algoritmo evolutivo",
   "id": "879d488e680ede50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def EA(G, N, p_c, p_m, X, y, Phi, random_state, paciencia=PACIENCIA, min_delta=MIN_DELTA):\n",
    "    # Aserciones\n",
    "    assert N >= 2 and not N % 2, \"El tama√±o de la poblaci√≥n debe ser par y mayor que 1.\"\n",
    "    assert 0.0 <= p_c <= 1.0, \"La probablidad de cruce debe estar entre 0 y 1.\"\n",
    "    assert 0.0 <= p_m <= 1.0, \"La probablidad de mutaci√≥n debe estar entre 0 y 1.\"\n",
    "\n",
    "    # ------- SPLIT -------\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Problema con todo precomputado y estructuras reutilizables\n",
    "    problem = Problem(X_train, y_train, X_val, y_val, Phi)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Crear poblaci√≥n inicial con N individuos\n",
    "    P = [Individuo(problem) for _ in range(N)]\n",
    "    InitializePopulation(P)\n",
    "\n",
    "    # Reparar y evaluar poblaci√≥n inicial\n",
    "    for I in P:\n",
    "        repair(I)\n",
    "        evaluate(I, problem)\n",
    "\n",
    "    # Crear poblaci√≥n auxilar\n",
    "    Q = [Individuo(problem) for _ in range(2 * N)]\n",
    "\n",
    "    # Evoluciona durante G generaciones (con early stopping)\n",
    "    best_ind = min(P)\n",
    "    best_outputs, progression_bar = IniciaProgreso(best_ind, G)\n",
    "\n",
    "    best_so_far = best_ind.f\n",
    "    last_improve_gen = 0\n",
    "\n",
    "    for gen in range(1, G + 1):\n",
    "        for i in range(N):\n",
    "            copia(Q[i], P[i])\n",
    "\n",
    "        i = N\n",
    "        while i < 2 * N:\n",
    "            copia(Q[i], binary_tournament_selection(P))\n",
    "            copia(Q[i + 1], binary_tournament_selection(P))\n",
    "\n",
    "            crossover(Q[i], Q[i + 1], p_c)\n",
    "            # None activa la regla de B√§ck: p_m = 1/L calculado por cada individuo\n",
    "            mutation(Q[i], None)\n",
    "            mutation(Q[i + 1], None)\n",
    "\n",
    "            repair(Q[i])\n",
    "            repair(Q[i + 1])\n",
    "\n",
    "            evaluate(Q[i], problem)\n",
    "            evaluate(Q[i + 1], problem)\n",
    "\n",
    "            i += 2\n",
    "\n",
    "        R = heapq.nsmallest(N, Q)\n",
    "        for i in range(N):\n",
    "            copia(P[i], R[i])\n",
    "\n",
    "        # actualizar progreso / curvas\n",
    "        ActualizaProgreso(P[0], gen, best_outputs, progression_bar)\n",
    "\n",
    "        # --- EARLY STOPPING ---\n",
    "        current_best = P[0].f\n",
    "        mejora = best_so_far - current_best\n",
    "\n",
    "        if mejora > min_delta:\n",
    "            best_so_far = current_best\n",
    "            last_improve_gen = gen\n",
    "        else:\n",
    "            waited = gen - last_improve_gen\n",
    "\n",
    "            if waited >= paciencia:\n",
    "                print(f\"üõë Parada Temprana en Gen {gen} (sin mejora > {min_delta} durante {paciencia} gens)\")\n",
    "                # Rellenar el resto del vector de progreso con el mejor valor para no romper plots\n",
    "                for g2 in range(gen + 1, G + 1):\n",
    "                    best_outputs[g2] = best_outputs[gen]\n",
    "                break\n",
    "\n",
    "    best_individuo = P[0]\n",
    "    return best_outputs, best_individuo.f, best_individuo"
   ],
   "id": "573c98ec4933bddc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sanity Check",
   "id": "28f83d8455839e7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# --- SANITY CHECK ---\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üîç SANITY CHECK: Verificando l√≥gica Voting vs Problem\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "name_test = list(files.keys())[0]\n",
    "X_check, y_check = LoadData(files[name_test])\n",
    "\n",
    "\n",
    "# 2. Cargar los modelos ajustados para ese dataset\n",
    "ruta_modelos_check = os.path.join(MODELOS_DIR, f\"{name_test}_best_models.pkl\")\n",
    "if not os.path.exists(ruta_modelos_check):\n",
    "    print(f\"‚ö†Ô∏è No se encontraron modelos ajustados para '{name_test}' en {ruta_modelos_check}.\")\n",
    "    print(\"   Ejecuta primero el notebook 00_1_ajuste_modelos.ipynb\")\n",
    "else:\n",
    "    modelos_dict_check = joblib.load(ruta_modelos_check)\n",
    "    if isinstance(modelos_dict_check, dict) and \"modelos\" in modelos_dict_check:\n",
    "        base_models_check = modelos_dict_check[\"modelos\"]\n",
    "        model_names_check = list(modelos_dict_check[\"nombres\"])\n",
    "    else:\n",
    "        base_models_check = modelos_dict_check\n",
    "        model_names_check = [f\"M{i}\" for i in range(len(base_models_check))]\n",
    "    print(f\"   Modelos cargados para '{name_test}': {[type(m).__name__ for m in base_models_check]}\")\n",
    "\n",
    "    indices = np.arange(len(X_check))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(len(X_check) * 0.8)\n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    X_tr_c, X_te_c = X_check[train_idx], X_check[test_idx]\n",
    "    y_tr_c, y_te_c = y_check[train_idx], y_check[test_idx]\n",
    "\n",
    "    phi_check = [clone(m) for m in base_models_check]\n",
    "    n_models  = len(phi_check)\n",
    "    n_features = X_tr_c.shape[1]\n",
    "\n",
    "    # VotingRegressor (Baseline)\n",
    "    vr = VotingRegressor([(f'm{i}', clone(base_models_check[i])) for i in range(n_models)])\n",
    "    vr.fit(X_tr_c, y_tr_c)\n",
    "    pred_vr = vr.predict(X_te_c)\n",
    "    rmse_vr = np.sqrt(mean_squared_error(y_te_c, pred_vr))\n",
    "\n",
    "    # Problem.f con todos los modelos y todos los features activos\n",
    "    problem_check = Problem(X_tr_c, y_tr_c, X_te_c, y_te_c, phi_check)\n",
    "    vector_ones  = np.ones(n_models, dtype=int)\n",
    "    matriz_ones  = np.ones((n_features, n_models), dtype=int)\n",
    "    rmse_custom  = problem_check.f(matriz_ones, vector_ones)\n",
    "\n",
    "    print(f\"\\nResultados:\")\n",
    "    print(f\"   RMSE VotingRegressor: {rmse_vr:.6f}\")\n",
    "    print(f\"   RMSE Problem.f:       {rmse_custom:.6f}\")\n",
    "\n",
    "    if abs(rmse_vr - rmse_custom) < 1e-5:\n",
    "        print(\"\\n‚úÖ √âXITO: ¬°Coinciden!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Diferencia de {abs(rmse_vr - rmse_custom):.6f}. Revisa Problem.f\")"
   ],
   "id": "29fffed8ab5bc14e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Correlaci√≥n entre los modelos elegidos",
   "id": "7f6295f146267107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä DIVERSIDAD: Verificando correlaci√≥n de predicciones\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "preds_dict = {}\n",
    "rmse_scores = {}\n",
    "\n",
    "print(f\"Entrenando modelos de '{name_test}' en split de prueba...\")\n",
    "\n",
    "for i, m_template in enumerate(base_models_check):\n",
    "    m = clone(m_template)\n",
    "    m.fit(X_tr_c, y_tr_c)\n",
    "    pred = m.predict(X_te_c)\n",
    "    label = model_names_check[i] if i < len(model_names_check) else f\"M{i}\"\n",
    "    preds_dict[label]  = pred\n",
    "    rmse_scores[label] = np.sqrt(mean_squared_error(y_te_c, pred))\n",
    "\n",
    "df_preds    = pd.DataFrame(preds_dict)\n",
    "corr_matrix = df_preds.corr()\n",
    "\n",
    "print(\"\\n--- 1. Rendimiento Individual (RMSE) ---\")\n",
    "for label, rmse in rmse_scores.items():\n",
    "    print(f\"   {label}: {rmse:.4f}\")\n",
    "\n",
    "print(\"\\n--- 2. Matriz de Correlaci√≥n (Pearson) ---\")\n",
    "print(corr_matrix)\n",
    "\n",
    "mask = np.ones(corr_matrix.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "mean_corr = corr_matrix.values[mask].mean()\n",
    "print(f\"\\nüîπ Correlaci√≥n Promedio del Pool: {mean_corr:.4f}\")\n",
    "\n",
    "if mean_corr > 0.96:\n",
    "    print(\"‚ö†Ô∏è ALERTA: Correlaci√≥n MUY ALTA (>0.96). Los modelos son casi id√©nticos.\")\n",
    "elif mean_corr < 0.80:\n",
    "    print(\"‚úÖ EXCELENTE: Correlaci√≥n baja (<0.80). El ensemble funcionar√° muy bien.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è NORMAL: Correlaci√≥n moderada (0.80 - 0.96).\")\n",
    "    print(\"   (En regresi√≥n es normal que sea alta porque todos intentan predecir el mismo 'y')\")"
   ],
   "id": "e2f6552d73a29b72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detector de Complementariedad",
   "id": "281b64e0da6e146a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def analizar_complementariedad(y_true, preds_dict):\n",
    "    \"\"\"\n",
    "    Versi√≥n mejorada: Calcula el Or√°culo y genera gr√°ficos de TODOS los pares de modelos.\n",
    "    \"\"\"\n",
    "    model_names = list(preds_dict.keys())\n",
    "    n_models = len(model_names)\n",
    "\n",
    "    # --- 1. C√ÅLCULO NUM√âRICO (Igual que antes) ---\n",
    "    squared_errors = np.zeros((len(y_true), n_models))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üîç AN√ÅLISIS DE COMPLEMENTARIEDAD (OR√ÅCULO)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, name in enumerate(model_names):\n",
    "        pred = preds_dict[name]\n",
    "        squared_errors[:, i] = (y_true - pred) ** 2\n",
    "        rmse = np.sqrt(np.mean(squared_errors[:, i]))\n",
    "        print(f\"   RMSE {name}: {rmse:.4f}\")\n",
    "\n",
    "    # Or√°culo\n",
    "    min_errors = np.min(squared_errors, axis=1)\n",
    "    oracle_rmse = np.sqrt(np.mean(min_errors))\n",
    "    best_single_rmse = np.min(np.sqrt(np.mean(squared_errors, axis=0)))\n",
    "    mejora_potencial = 100 * (1 - oracle_rmse / best_single_rmse)\n",
    "\n",
    "    print(f\"\\nüîÆ RMSE OR√ÅCULO: {oracle_rmse:.4f}\")\n",
    "    print(f\"üöÄ Margen de Mejora: {mejora_potencial:.2f}%\")\n",
    "\n",
    "    if mejora_potencial < 5.0:\n",
    "        print(\"‚ö†Ô∏è CONCLUSI√ìN: Poca complementariedad.\")\n",
    "    else:\n",
    "        print(\"‚úÖ CONCLUSI√ìN: Alta complementariedad.\")\n",
    "\n",
    "    # --- 2. VISUALIZACI√ìN DIN√ÅMICA (TODOS CONTRA TODOS) ---\n",
    "    if n_models >= 2:\n",
    "        # Generar todas las combinaciones posibles de pares (ej. A-B, A-C, B-C)\n",
    "        pairs = list(itertools.combinations(model_names, 2))\n",
    "        n_plots = len(pairs)\n",
    "\n",
    "        # Crear figura con subgr√°ficos\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5))\n",
    "        if n_plots == 1: axes = [axes] # Parche por si solo hay 2 modelos\n",
    "\n",
    "        for ax, (m1, m2) in zip(axes, pairs):\n",
    "            # Calcular residuos (errores con signo)\n",
    "            err1 = y_true - preds_dict[m1]\n",
    "            err2 = y_true - preds_dict[m2]\n",
    "\n",
    "            # Scatter plot\n",
    "            ax.scatter(err1, err2, alpha=0.4, s=15, c='blue', edgecolors='none')\n",
    "\n",
    "            # L√≠neas de referencia (cero)\n",
    "            ax.axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "            ax.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "            # L√≠nea de \"Fallo Id√©ntico\" (Diagonal roja)\n",
    "            # Si los puntos est√°n en esta l√≠nea, los modelos fallan igual (malo)\n",
    "            lims = [\n",
    "                np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "                np.max([ax.get_xlim(), ax.get_ylim()])\n",
    "            ]\n",
    "            ax.plot(lims, lims, 'r--', alpha=0.8, label=\"Correlaci√≥n Total\")\n",
    "\n",
    "            # Est√©tica\n",
    "            ax.set_xlabel(f\"Residuos {m1}\")\n",
    "            ax.set_ylabel(f\"Residuos {m2}\")\n",
    "            ax.set_title(f\"{m1} vs {m2}\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "analizar_complementariedad(y_te_c, preds_dict)"
   ],
   "id": "5c5dd876dbe3a783",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ejecuci√≥n del algoritmo evolutivo",
   "id": "38df31140ae4ab31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "best_runs_per_dataset = {}\n",
    "for name, path in files.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîµ PROCESANDO: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è Archivo no encontrado: {path}\")\n",
    "        continue\n",
    "\n",
    "    # --- CARGAR MODELOS AJUSTADOS PARA ESTE DATASET ---\n",
    "    ruta_modelos = os.path.join(MODELOS_DIR, f\"{name}_best_models.pkl\")\n",
    "    if not os.path.exists(ruta_modelos):\n",
    "        print(f\"‚ö†Ô∏è No se encontraron modelos ajustados para '{name}' en {ruta_modelos}\")\n",
    "        print(f\"   Ejecuta primero 00_1_ajuste_modelos.ipynb y vuelve a intentarlo.\")\n",
    "        continue\n",
    "    modelos_dict = joblib.load(ruta_modelos)\n",
    "    if isinstance(modelos_dict, dict) and \"modelos\" in modelos_dict:\n",
    "        modelos_base = modelos_dict[\"modelos\"]\n",
    "        model_names = list(modelos_dict[\"nombres\"])\n",
    "    else:\n",
    "        modelos_base = modelos_dict\n",
    "        model_names = [f\"M{i}\" for i in range(len(modelos_base))]\n",
    "    print(f\"   Modelos cargados: {[type(m).__name__ for m in modelos_base]}\")\n",
    "\n",
    "    X, y = LoadData(path)\n",
    "    if X is None: continue\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    dataset_results = []\n",
    "    min_rmse_dataset = float('inf')\n",
    "    best_run_data = None\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    print(f\"   Iniciando 10-Fold CV...\")\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Clonar los modelos ajustados de este dataset para el EA\n",
    "        Phi = [clone(m) for m in modelos_base]\n",
    "\n",
    "        # --- A. EVALUAR CADA MODELO BASE POR SEPARADO ---\n",
    "        fold_baselines = {}\n",
    "        for m_name, m_template in zip(model_names, modelos_base):\n",
    "            try:\n",
    "                m_single = clone(m_template)\n",
    "                m_single.fit(X_train, y_train)\n",
    "                pred_single = m_single.predict(X_test)\n",
    "                rmse_single = np.sqrt(mean_squared_error(y_test, pred_single))\n",
    "                fold_baselines[f\"RMSE_{m_name}\"] = rmse_single\n",
    "            except Exception:\n",
    "                fold_baselines[f\"RMSE_{m_name}\"] = float('inf')\n",
    "\n",
    "        local_best_base = min(fold_baselines.values())\n",
    "\n",
    "        # --- B. EJECUTAR EA ---\n",
    "        start_t = time.time()\n",
    "        best_outputs, best_f, best_solution = EA(\n",
    "            G, N, p_c, p_m,\n",
    "            X_train, y_train,\n",
    "            Phi,\n",
    "            random_state=42\n",
    "        )\n",
    "        elapsed = time.time() - start_t\n",
    "\n",
    "        # --- C. EVALUAR EN TEST ---\n",
    "        problem_test = Problem(X_train, y_train, X_test, y_test, Phi)\n",
    "        test_rmse_ea = problem_test.f(best_solution.matriz, best_solution.vector)\n",
    "\n",
    "        # Usar los nombres correctos para imprimir resultados\n",
    "        print_str = f\"   F{fold_idx}: EA={test_rmse_ea:.4f}\"\n",
    "        for m_name in model_names:\n",
    "            val = fold_baselines.get(f'RMSE_{m_name}', 99)\n",
    "            print_str += f\" | {m_name}={val:.4f}\"\n",
    "        print(print_str)\n",
    "\n",
    "        if test_rmse_ea < min_rmse_dataset:\n",
    "            min_rmse_dataset = test_rmse_ea\n",
    "            best_run_data = {\n",
    "                'fold':      fold_idx,\n",
    "                'solution':  best_solution,\n",
    "                'outputs':   best_outputs,\n",
    "                'f_val':     best_f,\n",
    "                'modelos_base': modelos_base,   # guardamos los modelos de este dataset\n",
    "                'model_names': model_names,\n",
    "                'X_train': X_train, 'y_train': y_train,\n",
    "                'X_test':  X_test,  'y_test':  y_test\n",
    "            }\n",
    "\n",
    "        row_data = {\n",
    "            'Dataset':       name,\n",
    "            'Fold':          fold_idx,\n",
    "            'RMSE_EA_Test':  test_rmse_ea,\n",
    "            'RMSE_EA_Val':   best_f,\n",
    "            'RMSE_Best_Base': local_best_base,\n",
    "            'N_Modelos':     np.sum(best_solution.vector),\n",
    "            'N_Features':    np.sum(best_solution.matriz),\n",
    "            'Time_s':        elapsed\n",
    "        }\n",
    "        row_data.update(fold_baselines)\n",
    "        dataset_results.append(row_data)\n",
    "\n",
    "    # --- FIN BUCLE FOLDS ---\n",
    "    df_temp = pd.DataFrame(dataset_results)\n",
    "    means   = df_temp.mean(numeric_only=True)\n",
    "\n",
    "    # Usar los nombres correctos para la comparativa justa\n",
    "    mean_bases = [means.get(f'RMSE_{m}', float('inf')) for m in model_names]\n",
    "    fair_best_base = min(mean_bases)\n",
    "\n",
    "    summary_row = {\n",
    "        'Dataset': 'MEDIA', 'Fold': '-',\n",
    "        'RMSE_EA_Test':  means['RMSE_EA_Test'],\n",
    "        'RMSE_EA_Val':   means['RMSE_EA_Val'],\n",
    "        'RMSE_Best_Base': fair_best_base,\n",
    "        'N_Modelos':     means['N_Modelos'],\n",
    "        'N_Features':    means['N_Features'],\n",
    "        'Time_s':        means['Time_s']\n",
    "    }\n",
    "    for m, val in zip(model_names, mean_bases):\n",
    "        summary_row[f'RMSE_{m}'] = val\n",
    "    df_final = pd.concat([df_temp, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "    print(f\"\\n   ‚öñÔ∏è COMPARATIVA JUSTA ({name}):\")\n",
    "    for m, val in zip(model_names, mean_bases):\n",
    "        print(f\"      {m} (Avg): {val:.4f}\")\n",
    "    print(f\"      -----------------------------\")\n",
    "    print(f\"      üèÜ Mejor Base Global: {fair_best_base:.4f}\")\n",
    "    print(f\"      ü§ñ Tu Sistema (EA):   {means['RMSE_EA_Test']:.4f}\")\n",
    "\n",
    "    if means['RMSE_EA_Test'] < fair_best_base:\n",
    "        impr = 100 * (1 - means['RMSE_EA_Test'] / fair_best_base)\n",
    "        print(f\"      ‚úÖ RESULTADO: El EA gana por un {impr:.2f}%\")\n",
    "    else:\n",
    "        diff = 100 * (means['RMSE_EA_Test'] / fair_best_base - 1)\n",
    "        print(f\"      ‚ùå RESULTADO: El EA pierde por un {diff:.2f}% contra el especialista\")\n",
    "\n",
    "    csv_path = os.path.join(RESULTS_DIR, f\"{name}_results.csv\")\n",
    "    df_final.to_csv(csv_path, index=False)\n",
    "    print(f\"   üíæ Guardado: {csv_path}\")\n",
    "\n",
    "    if best_run_data:\n",
    "        best_runs_per_dataset[name] = best_run_data\n",
    "\n",
    "print(f\"\\nüöÄ ¬°Ejecuci√≥n completa! Revisa la carpeta {RESULTS_DIR}\")"
   ],
   "id": "11ef1670aa03583b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imprimir los algoritmos de aprendizaje seleccionados, los atributos seleccionados para cada algoritmo de aprendizaje y el fitness (rmse en un conjunto de validaci√≥n interno)",
   "id": "d96cdc2c41cd586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def PrintSolution(I, Phi):\n",
    "    Phi_index = np.where(I.vector == 1)[0]\n",
    "    for index in Phi_index:\n",
    "        attributes_selected = [l for l in range(I.matriz.shape[0]) if I.matriz[l,index] == 1]\n",
    "        print(f\"   ü§ñ Modelo: {Phi[index]}\")\n",
    "        print(f\"      Atributos: {attributes_selected}\")"
   ],
   "id": "888b3bcb99334316",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizaci√≥n de la evoluci√≥n del algoritmo",
   "id": "48a2038230bc70fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def VisualizaEvolucion(best_outputs, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.set_title(f\"Evoluci√≥n - {title}\")\n",
    "    plt.plot(best_outputs)\n",
    "    plt.xlabel(\"Generaci√≥n\")\n",
    "    plt.ylabel(\"Fitness (RMSE Val)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ],
   "id": "edb6b6c69dad4b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Resultados en el conjunto de test",
   "id": "4f8b61b7415b22ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "for name, data in best_runs_per_dataset.items():\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"üìä REPORTE VISUAL PARA: {name.upper()}\")\n",
    "    print(f\"   (Mejor Fold encontrado: {data['fold']})\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "\n",
    "    best_solution = data['solution']\n",
    "    best_outputs  = data['outputs']\n",
    "    best_f        = data['f_val']\n",
    "    modelos_base  = data['modelos_base']   # modelos ajustados espec√≠ficos del dataset\n",
    "    model_names   = data.get('model_names', [f\"M{i}\" for i in range(len(modelos_base))])\n",
    "    X_train, y_train = data['X_train'], data['y_train']\n",
    "    X_test,  y_test  = data['X_test'], data['y_test']\n",
    "\n",
    "    print(\"--- üß¨ Estructura del Mejor Individuo (Ensemble) ---\")\n",
    "    PrintSolution(best_solution, model_names)\n",
    "    print(f\"\\nüìâ RMSE Validaci√≥n Interna (Fitness): {best_f:.4f}\")\n",
    "\n",
    "    VisualizaEvolucion(best_outputs, title=f\"{name} (Fold {data['fold']})\")\n",
    "\n",
    "    # Clonar los modelos ajustados de este dataset para evaluar en test\n",
    "    Phi_test     = [clone(m) for m in modelos_base]\n",
    "    problem_test = Problem(X_train, y_train, X_test, y_test, Phi_test)\n",
    "    rmse_test    = problem_test.f(best_solution.matriz, best_solution.vector)\n",
    "\n",
    "    print(f\"üèÅ RMSE TEST FINAL ({name}): {rmse_test:.4f}\")\n",
    "    print(\"-\" * 80)\n"
   ],
   "id": "57fc1d9331d47bcb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
