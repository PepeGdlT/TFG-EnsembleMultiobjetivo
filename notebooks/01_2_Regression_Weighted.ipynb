{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d130f4e0a8fb20c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1c0b6b8",
   "metadata": {},
   "source": [
    "### Requerimientos"
   ]
  },
  {
   "cell_type": "code",
   "id": "daca297d",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from pandas.core.common import random_state\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import arff\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "022debc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Lectura de los datos\n",
    "#### Input:\n",
    "  - $file\\_path$: Nombre completo con path de la base de datos .arff a cargar\n",
    "  \n",
    "#### Output:\n",
    "  - $X$: Atributos de entrada num√©ricos\n",
    "  - $y$: Salida num√©rica"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd8131d2-cdc3-48ca-a25e-8c8a00a0002c",
   "metadata": {},
   "source": [
    "import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def LoadData(file_path):\n",
    "    # 1. Cargar con liac-arff\n",
    "    with open(file_path, 'r') as f:\n",
    "        dataset = arff.load(f)\n",
    "\n",
    "    col_names = [attr[0] for attr in dataset['attributes']]\n",
    "    df = pd.DataFrame(dataset['data'], columns=col_names)\n",
    "\n",
    "    # 2. Limpieza b√°sica\n",
    "    df.replace([None], np.nan, inplace=True)\n",
    "    filename = file_path.lower()\n",
    "\n",
    "    # --- CORRECCIONES ESPEC√çFICAS ---\n",
    "\n",
    "    # CASO US CRIME: Eliminar ID in√∫til\n",
    "    if 'crime' in filename and 'communityname' in df.columns:\n",
    "        df = df.drop(columns=['communityname'])\n",
    "\n",
    "    # CASO BOSTON: Arreglar columnas que cargan mal\n",
    "    if 'boston' in filename:\n",
    "        cols_to_fix = ['CHAS', 'RAD']\n",
    "        for col in cols_to_fix:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 3. FORZADO NUM√âRICO (SOLO REGRESI√ìN)\n",
    "    # Convertimos todo a n√∫meros.\n",
    "    # En Abalone, 'Sex' se convertir√° enteramente en NaNs.\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # 4. BORRAR COLUMNAS VAC√çAS (EL FIX PARA EL ERROR)\n",
    "    # Si una columna es 100% NaN (como 'Sex' en Abalone), la borramos YA.\n",
    "    # As√≠ el imputer no se queja y los tama√±os coinciden.\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # 5. Imputaci√≥n de nulos restantes\n",
    "    # Si quedan huecos sueltos, los rellenamos con la media\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        # Usamos un DataFrame nuevo para evitar conflictos de √≠ndices\n",
    "        data_imputed = imputer.fit_transform(df)\n",
    "        df = pd.DataFrame(data_imputed, columns=df.columns)\n",
    "\n",
    "    # 6. Separar X e y\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:, -1].values\n",
    "\n",
    "    return X, y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da36d2bb-7eb4-4334-ac9e-a1ceedf7f98d",
   "metadata": {},
   "source": [
    "### Par√°metros del algoritmo evolutivo\n",
    "- $G$: N√∫mero de generaciones\n",
    "- $N$: Tama√±o de la poblaci√≥n\n",
    "- $p\\_c$: Probabilidad de cruce\n",
    "- $p\\_m$: Probabilidad de mutaci√≥n\n",
    "- $random\\_state$: Semilla para reproducibilidad\n",
    "- $X,y$: Datos\n",
    "- $Phi$: Algoritmos de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d6a3f45",
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, KBinsDiscretizer\n",
    "\n",
    "# --- 1. PAR√ÅMETROS GEN√âTICOS ---\n",
    "# G pasa a ser el techo real de generaciones; el EA har√° early-stopping antes si converge.\n",
    "MAX_GEN = 150\n",
    "PACIENCIA = 25\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "G = MAX_GEN\n",
    "N = 50\n",
    "p_c = 0.85\n",
    "p_m = 0.2\n",
    "\n",
    "# --- 2. RUTAS ---\n",
    "DATA_DIR = \"../data/regression\"\n",
    "RESULTS_DIR = \"../results/regression-weighted\"\n",
    "MODELOS_DIR = \"../modelos_ajustados\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    'Boston':    os.path.join(DATA_DIR, 'boston.arff'),\n",
    "    'Concrete':  os.path.join(DATA_DIR, 'concrete.arff'),\n",
    "    'US_Crime':  os.path.join(DATA_DIR, 'us_crime.arff'),\n",
    "    'Abalone':   os.path.join(DATA_DIR, 'abalone.arff'),\n",
    "    'Elevators': os.path.join(DATA_DIR, 'elevators.arff'),\n",
    "}\n",
    "\n",
    "# Nombres de los modelos base (en el mismo orden en que fueron guardados por 00_1_ajuste_modelos)\n",
    "model_names = [\"ET\", \"RIDGE\", \"KNN\"]\n",
    "\n",
    "print(\"‚úÖ Configuraci√≥n cargada.\")\n",
    "print(f\"   Modelos ajustados en: {os.path.abspath(MODELOS_DIR)}\")\n",
    "print(f\"   Resultados ir√°n a:    {os.path.abspath(RESULTS_DIR)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Definici√≥n del problema de optimizaci√≥n\n",
    "\n",
    "$$\\textit{Minimizar} \\ f(\\mathbf{x}) = RMSE(\\hat{y}_{ensemble}, D_{val})$$\n",
    "\n",
    "El vector de decisi√≥n es $\\mathbf{x}=(\\mathbf{M}, \\mathbf{v}, \\mathbf{w})$, donde:\n",
    "\n",
    "* $\\mathbf{M}_{j,i} \\in \\{0,1\\}$: Indica si el atributo $j$ se ha seleccionado para el algoritmo de aprendizaje $i$.\n",
    "* $\\mathbf{v}_i \\in \\{0,1\\}$: Indica si el algoritmo de aprendizaje $i$ est√° activo (seleccionado).\n",
    "* $\\mathbf{w}_i \\in [0,1]$: **Peso** asociado al algoritmo de aprendizaje $i$ para la votaci√≥n ponderada.\n",
    "\n",
    "La predicci√≥n final del ensemble se calcula normalizando los pesos de los modelos activos:\n",
    "\n",
    "$$\\hat{y}_{ensemble} = \\sum_{i \\in Active} \\left( \\frac{w_i}{\\sum_{k \\in Active} w_k} \\right) \\cdot h_i(X)$$\n",
    "\n",
    "Donde:\n",
    "* $D_{train}=(X_{train},y_{train})$: Conjunto de datos de entrenamiento.\n",
    "* $D_{val}=(X_{val},y_{val})$: Conjunto de datos de validaci√≥n.\n",
    "* $\\Phi$: Conjunto de algoritmos de aprendizaje base."
   ],
   "id": "e47926ea9fc6e4f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "class Problem:\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, Phi): \n",
    "        \n",
    "        # ------- DATOS --------\n",
    "        self.X_train = np.asarray(X_train)\n",
    "        self.y_train = np.asarray(y_train)\n",
    "        self.X_val   = np.asarray(X_val)\n",
    "        self.y_val   = np.asarray(y_val)\n",
    "\n",
    "        # ------- MODELOS --------\n",
    "        self.Phi = Phi\n",
    "\n",
    "        # ------- DIMENSIONES -------\n",
    "        self.n = self.X_train.shape[1]     # n¬∫ de atributos\n",
    "        self.m = len(self.Phi)             # n¬∫ modelos base\n",
    "\n",
    "        # ------- BUFFERS REUTILIZABLES (Optimizaci√≥n de memoria) -------\n",
    "        self.X_train_selected = np.empty((self.X_train.shape[0], self.n), dtype=self.X_train.dtype)\n",
    "        self.X_val_selected   = np.empty((self.X_val.shape[0],   self.n), dtype=self.X_val.dtype)\n",
    "        self.y_val_pred       = np.empty(self.y_val.shape[0], dtype=self.y_val.dtype)\n",
    "\n",
    "    # =====================================================================================\n",
    "    #                               FUNCI√ìN DE EVALUACI√ìN (Fitness)\n",
    "    # =====================================================================================\n",
    "    def f(self, matriz, vector, pesos):\n",
    "        \"\"\"\n",
    "        Calcula el RMSE del ensemble seleccionado por el individuo con WEIGHTED VOTING.\n",
    "        \"\"\"\n",
    "\n",
    "        # √≠ndices de modelos activos\n",
    "        vector_index = np.flatnonzero(vector)\n",
    "        n_selected = len(vector_index)\n",
    "\n",
    "        # 1. PROTECCI√ìN: Si no hay modelos seleccionados, devolvemos error infinito\n",
    "        if n_selected == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        self.y_val_pred[:] = 0\n",
    "\n",
    "        # Extraer pesos de modelos activos\n",
    "        pesos_activos = pesos[vector_index]\n",
    "        suma_pesos = np.sum(pesos_activos)\n",
    "\n",
    "        # 2. PROTECCI√ìN: Si todos los pesos son cero, fallback a infinito\n",
    "        if suma_pesos == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        # Normalizar pesos\n",
    "        pesos_norm = pesos_activos / suma_pesos\n",
    "\n",
    "        for k, index in enumerate(vector_index):\n",
    "            # Selecci√≥n de columnas (Features)\n",
    "            cols_selected = np.flatnonzero(matriz[:, index])\n",
    "            p = len(cols_selected)\n",
    "\n",
    "            # Si el modelo no tiene features seleccionadas, usamos todas (Fallback)\n",
    "            if p == 0:\n",
    "                cols_selected = np.arange(self.n)\n",
    "                p = self.n\n",
    "\n",
    "            # Copia r√°pida de datos usando buffers\n",
    "            X_train_selected = self.X_train_selected[:, :p]\n",
    "            np.take(self.X_train, cols_selected, axis=1, out=X_train_selected)\n",
    "\n",
    "            X_val_selected = self.X_val_selected[:, :p]\n",
    "            np.take(self.X_val, cols_selected, axis=1, out=X_val_selected)\n",
    "\n",
    "            # 2. IMPORTANTE: Clonar modelo para entrenar desde cero\n",
    "            model = clone(self.Phi[index])\n",
    "            model.fit(X_train_selected, self.y_train)\n",
    "\n",
    "            # 3. WEIGHTED VOTING: Suma ponderada en lugar de promedio uniforme\n",
    "            self.y_val_pred[:] += pesos_norm[k] * model.predict(X_val_selected)\n",
    "\n",
    "\n",
    "\n",
    "        # Ya no dividimos porque los pesos est√°n normalizados\n",
    "        rmse = np.sqrt(mean_squared_error(self.y_val, self.y_val_pred))\n",
    "\n",
    "        return rmse"
   ],
   "id": "38f2fde51f884eb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Definici√≥n de individuo",
   "id": "ba000c2136ec188d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Individuo:\n",
    "    def __init__(self,problem):\n",
    "        self.matriz = np.zeros((problem.n, problem.m), dtype=int) # selecci√≥n de atributos para cada algoritmo de aprendizaje\n",
    "        self.vector = np.zeros(problem.m, dtype=int)  # selecci√≥n de algoritmos de aprendizaje\n",
    "        self.pesos = np.zeros(problem.m, dtype=float)  # WEIGHTED: pesos por modelo\n",
    "        self.f = 0.0\n",
    "    def __lt__(self, ind):\n",
    "        return self.f < ind.f"
   ],
   "id": "c23fd2189aaabfae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inicializaci√≥n de la poblaci√≥n",
   "id": "501470ba6eabdea6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def InitializePopulation(P):\n",
    "    for I in P:\n",
    "        I.matriz[:] = np.random.randint(0, 2, size=I.matriz.shape)\n",
    "        I.vector[:] = np.random.randint(0, 2, size=I.vector.size)\n",
    "        # WEIGHTED\n",
    "        I.pesos[:] = np.random.uniform(0, 1, size=I.pesos.size)\n",
    "### Funci√≥n de reparo"
   ],
   "id": "a4f5ac6024f31d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def repair(I):\n",
    "\n",
    "    n = I.matriz.shape[0]\n",
    "    m = I.matriz.shape[1]\n",
    "\n",
    "    n_selected = I.vector.sum()\n",
    "\n",
    "    # --- 1) garantizar >= 2 modelos base ---\n",
    "    if n_selected == 0:\n",
    "        idx = np.random.choice(m, 2, replace=False)\n",
    "        I.vector[:] = 0\n",
    "        I.vector[idx] = 1\n",
    "\n",
    "    elif n_selected == 1:\n",
    "        selected_idx = np.flatnonzero(I.vector)[0]\n",
    "        rem = np.delete(np.arange(m), selected_idx)\n",
    "        I.vector[np.random.choice(rem)] = 1\n",
    "\n",
    "    # √≠ndices de modelos base activos\n",
    "    Phi_index = np.flatnonzero(I.vector)\n",
    "\n",
    "    # --- 2) garantizar ‚â•1 atributo por base seleccionada ---\n",
    "    cols_empty = Phi_index[np.sum(I.matriz[:, Phi_index], axis=0) == 0]\n",
    "    if len(cols_empty) > 0:\n",
    "        rand_rows = np.random.randint(0, n, size=len(cols_empty))\n",
    "        I.matriz[rand_rows, cols_empty] = 1  # in-place\n",
    "\n",
    "    # --- 3) WEIGHTED: reparar pesos en [0, 1] ---\n",
    "    # Poner a cero los pesos de modelos no seleccionados\n",
    "    mask_off = I.vector == 0\n",
    "    I.pesos[mask_off] = 0.0\n",
    "\n",
    "    # Asegurar que los pesos activos sean no negativos y <= 1\n",
    "    I.pesos[Phi_index] = np.clip(I.pesos[Phi_index], 0.0, 1.0)\n",
    "\n",
    "    # Si todos los pesos activos son cero, inicializar uniformemente a 1/m_activos\n",
    "    # (fallback: no deber√≠a ocurrir salvo en la inicializaci√≥n o tras mutaci√≥n extrema)\n",
    "    if np.sum(I.pesos[Phi_index]) == 0:\n",
    "        I.pesos[Phi_index] = 1.0 / len(Phi_index)\n",
    "\n",
    "    # NO se normaliza aqu√≠: Problem.f lo hace internamente,\n",
    "    # y normalizar en repair colapsar√≠a el espacio de b√∫squeda de SBX/PM"
   ],
   "id": "21594b95f4d59291",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Funci√≥n de evaluaci√≥n",
   "id": "b815b512ae3e7a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate(I,problem):\n",
    "    I.f = problem.f(I.matriz, I.vector, I.pesos)"
   ],
   "id": "74c8ff6a9aeaf406",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Copia de individuo",
   "id": "69a0c4e4bc82f6c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def copia(I1, I2):\n",
    "    I1.matriz[:] = I2.matriz\n",
    "    I1.vector[:] = I2.vector\n",
    "    I1.pesos[:] = I2.pesos  # WEIGHTED\n",
    "    I1.f = I2.f"
   ],
   "id": "49146dc683f4a39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Selecci√≥n por torneo binario",
   "id": "3ad3e8b622c791e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def binary_tournament_selection(P):\n",
    "    return min(np.random.choice(P,2,replace=False))"
   ],
   "id": "597a70ac0686030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cruce\n",
    "- Cruce uniforme con probabilidad $p\\_c$"
   ],
   "id": "71b501db1e955c2e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def crossover(I1,I2,p_c):\n",
    "    if np.random.random()<=p_c:\n",
    "        for l in range(I1.vector.size):\n",
    "            if np.random.random()<=0.5:\n",
    "                I1.vector[l], I2.vector[l] = I2.vector[l], I1.vector[l]\n",
    "        for l1 in range(I1.matriz.shape[0]):\n",
    "            for l2 in range(I1.matriz.shape[1]):\n",
    "                if np.random.random()<=0.5:\n",
    "                    I1.matriz[l1][l2], I2.matriz[l1][l2] = I2.matriz[l1][l2], I1.matriz[l1][l2]\n",
    "        # SBX para pesos (variables continuas en [0, 1])\n",
    "        eta_c = 15.0\n",
    "        for l in range(I1.pesos.size):\n",
    "            if np.random.random() <= 0.5:\n",
    "                x1, x2 = I1.pesos[l], I2.pesos[l]\n",
    "                if abs(x1 - x2) > 1e-10:\n",
    "                    if x1 > x2:\n",
    "                        x1, x2 = x2, x1\n",
    "                    beta = 1.0 + (2.0 * x1) / (x2 - x1)\n",
    "                    alpha = 2.0 - beta ** (-(eta_c + 1.0))\n",
    "                    u = np.random.random()\n",
    "                    if u <= 1.0 / alpha:\n",
    "                        beta_q = (u * alpha) ** (1.0 / (eta_c + 1.0))\n",
    "                    else:\n",
    "                        beta_q = (1.0 / (2.0 - u * alpha)) ** (1.0 / (eta_c + 1.0))\n",
    "                    c1 = 0.5 * ((x1 + x2) - beta_q * (x2 - x1))\n",
    "                    c2 = 0.5 * ((x1 + x2) + beta_q * (x2 - x1))\n",
    "                    I1.pesos[l] = np.clip(c1, 0.0, 1.0)\n",
    "                    I2.pesos[l] = np.clip(c2, 0.0, 1.0)"
   ],
   "id": "6c9bfdea0bb01b8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mutaci√≥n\n",
    "- Mutaci√≥n din√°mica (B√§ck): $p_m = 1/L$ donde $L$ es el tama√±o total del cromosoma\n",
    "- Si se pasa un valor fijo de $p\\_m$ se usa ese; si es `None` se aplica la regla de B√§ck"
   ],
   "id": "7666743b08ce0077"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def mutation(I, p_m=None):\n",
    "    # --- Regla de B√§ck: mutaci√≥n din√°mica ---\n",
    "    # L = total de genes del cromosoma (bits del vector + bits de la matriz + reales de pesos)\n",
    "    # p_m = 1/L garantiza que, en media, se mute exactamente 1 gen por individuo,\n",
    "    # independientemente del tama√±o del problema (clave para datasets de 126 features).\n",
    "    if p_m is None:\n",
    "        L = I.vector.size + I.matriz.size + I.pesos.size\n",
    "        p_m = 1.0 / max(1, L)\n",
    "\n",
    "    for l in range(I.vector.size):\n",
    "        if np.random.random() <= p_m:\n",
    "            I.vector[l] = np.random.randint(2)\n",
    "    for l1 in range(I.matriz.shape[0]):\n",
    "        for l2 in range(I.matriz.shape[1]):\n",
    "            if np.random.random() <= p_m:\n",
    "                I.matriz[l1][l2] = np.random.randint(2)\n",
    "    # Mutaci√≥n Polinomial para pesos (variables continuas en [0, 1])\n",
    "    eta_m = 20.0\n",
    "    for l in range(I.pesos.size):\n",
    "        if np.random.random() <= p_m:\n",
    "            x = I.pesos[l]\n",
    "            u = np.random.random()\n",
    "            if u < 0.5:\n",
    "                delta = (2.0 * u) ** (1.0 / (eta_m + 1.0)) - 1.0\n",
    "            else:\n",
    "                delta = 1.0 - (2.0 * (1.0 - u)) ** (1.0 / (eta_m + 1.0))\n",
    "            I.pesos[l] = np.clip(x + delta, 0.0, 1.0)"
   ],
   "id": "a7cf974f617c6a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Progreso del algoritmo",
   "id": "afc481023584e189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def IniciaProgreso(best_individuo,G):\n",
    "    best_outputs = [0]*(G+1) # Para visualizar la gr√°fica de evoluci√≥n al final\n",
    "    progression_bar = tqdm(total=G, leave=False)\n",
    "    ActualizaProgreso(best_individuo,0,best_outputs,progression_bar)\n",
    "    return best_outputs,progression_bar\n",
    "\n",
    "def ActualizaProgreso(best_individuo,gen,best_outputs,progression_bar):\n",
    "    best_fitness = best_individuo.f\n",
    "    best_rmse = best_fitness\n",
    "    progression_bar.set_description(\"Generation: %s RMSE: %s \" % (str(gen), str(best_rmse)))\n",
    "    best_outputs[gen] = best_fitness # A√±adir mejor fitness (para visualizaci√≥n)\n",
    "    progression_bar.update(1)"
   ],
   "id": "582c83c77c793020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Algoritmo evolutivo",
   "id": "46f098c72c130175"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def EA(G, N, p_c, p_m, X, y, Phi, random_state, paciencia=PACIENCIA, min_delta=MIN_DELTA):\n",
    "    # Aserciones\n",
    "    assert N >= 2 and not N % 2, \"El tama√±o de la poblaci√≥n debe ser par y mayor que 1.\"\n",
    "    assert 0.0 <= p_c <= 1.0, \"La probablidad de cruce debe estar entre 0 y 1.\"\n",
    "    assert 0.0 <= p_m <= 1.0, \"La probablidad de mutaci√≥n debe estar entre 0 y 1.\"\n",
    "\n",
    "    # ------- SPLIT -------\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Problema con todo precomputado y estructuras reutilizables\n",
    "    problem = Problem(X_train, y_train, X_val, y_val, Phi)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Crear poblaci√≥n inicial con N individuos\n",
    "    P = [Individuo(problem) for _ in range(N)]\n",
    "    InitializePopulation(P)\n",
    "\n",
    "    # Reparar y evaluar poblaci√≥n inicial\n",
    "    for I in P:\n",
    "        repair(I)\n",
    "        evaluate(I, problem)\n",
    "\n",
    "    # Crear poblaci√≥n auxilar\n",
    "    Q = [Individuo(problem) for _ in range(2 * N)]\n",
    "\n",
    "    # Evoluciona durante G generaciones (con early stopping)\n",
    "    best_ind = min(P)\n",
    "    best_outputs, progression_bar = IniciaProgreso(best_ind, G)\n",
    "\n",
    "    best_so_far = best_ind.f\n",
    "    last_improve_gen = 0\n",
    "\n",
    "    for gen in range(1, G + 1):\n",
    "        for i in range(N):\n",
    "            copia(Q[i], P[i])\n",
    "\n",
    "        i = N\n",
    "        while i < 2 * N:\n",
    "            copia(Q[i], binary_tournament_selection(P))\n",
    "            copia(Q[i + 1], binary_tournament_selection(P))\n",
    "\n",
    "            crossover(Q[i], Q[i + 1], p_c)\n",
    "            # None activa la regla de B√§ck: p_m = 1/L calculado por cada individuo\n",
    "            mutation(Q[i], None)\n",
    "            mutation(Q[i + 1], None)\n",
    "\n",
    "            repair(Q[i])\n",
    "            repair(Q[i + 1])\n",
    "\n",
    "            evaluate(Q[i], problem)\n",
    "            evaluate(Q[i + 1], problem)\n",
    "\n",
    "            i += 2\n",
    "\n",
    "        R = heapq.nsmallest(N, Q)\n",
    "        for i in range(N):\n",
    "            copia(P[i], R[i])\n",
    "\n",
    "        # actualizar progreso / curvas\n",
    "        ActualizaProgreso(P[0], gen, best_outputs, progression_bar)\n",
    "\n",
    "        # --- EARLY STOPPING ---\n",
    "        current_best = P[0].f\n",
    "        mejora = best_so_far - current_best\n",
    "\n",
    "        if mejora > min_delta:\n",
    "            best_so_far = current_best\n",
    "            last_improve_gen = gen\n",
    "        else:\n",
    "            waited = gen - last_improve_gen\n",
    "\n",
    "            if waited >= paciencia:\n",
    "                print(f\"üõë Parada Temprana en Gen {gen} (sin mejora > {min_delta} durante {paciencia} gens)\")\n",
    "                # Rellenar el resto del vector de progreso con el mejor valor para no romper plots\n",
    "                for g2 in range(gen + 1, G + 1):\n",
    "                    best_outputs[g2] = best_outputs[gen]\n",
    "                break\n",
    "\n",
    "    best_individuo = P[0]\n",
    "    return best_outputs, best_individuo.f, best_individuo"
   ],
   "id": "dda76f8af766fad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Sanity Check",
   "id": "9924a1afcf643ede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# --- SANITY CHECK MEJORADO ---\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üîç SANITY CHECK: Verificando l√≥gica Weighted Voting vs Problem\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# 1. Datos y Split\n",
    "name_test = list(files.keys())[0]\n",
    "X_check, y_check = LoadData(files[name_test])\n",
    "\n",
    "# 2. Cargar los modelos ajustados para ese dataset\n",
    "ruta_modelos_check = os.path.join(MODELOS_DIR, f\"{name_test}_best_models.pkl\")\n",
    "if not os.path.exists(ruta_modelos_check):\n",
    "    print(f\"‚ö†Ô∏è No se encontraron modelos ajustados para '{name_test}' en {ruta_modelos_check}.\")\n",
    "    print(\"   Ejecuta primero el notebook 00_1_ajuste_modelos.ipynb\")\n",
    "else:\n",
    "    modelos_dict_check = joblib.load(ruta_modelos_check)\n",
    "    if isinstance(modelos_dict_check, dict) and \"modelos\" in modelos_dict_check:\n",
    "        base_models_check = modelos_dict_check[\"modelos\"]\n",
    "        model_names_check = list(modelos_dict_check[\"nombres\"])\n",
    "    else:\n",
    "        base_models_check = modelos_dict_check\n",
    "        model_names_check = [f\"M{i}\" for i in range(len(base_models_check))]\n",
    "    print(f\"   Modelos cargados para '{name_test}': {[type(m).__name__ for m in base_models_check]}\")\n",
    "\n",
    "    indices = np.arange(len(X_check))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(len(X_check) * 0.8)\n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    X_tr_c, X_te_c = X_check[train_idx], X_check[test_idx]\n",
    "    y_tr_c, y_te_c = y_check[train_idx], y_check[test_idx]\n",
    "\n",
    "    phi_check = [clone(m) for m in base_models_check]\n",
    "    n_models  = len(phi_check)\n",
    "    n_features = X_tr_c.shape[1]\n",
    "    vector_ones = np.ones(n_models, dtype=int)\n",
    "    matriz_ones = np.ones((n_features, n_models), dtype=int)\n",
    "\n",
    "    def compare_with_weights(weights, tol=1e-6):\n",
    "        \"\"\"Compara VotingRegressor(weights) con Problem.f usando matriz y vector todos 1.\"\"\"\n",
    "        print(f\"\\n--- Probando pesos: {weights} ---\")\n",
    "        # VotingRegressor\n",
    "        vr = VotingRegressor(\n",
    "            [(f'm{i}', clone(base_models_check[i])) for i in range(n_models)],\n",
    "            weights=list(weights)\n",
    "        )\n",
    "        vr.fit(X_tr_c, y_tr_c)\n",
    "        pred_vr = vr.predict(X_te_c)\n",
    "        rmse_vr = np.sqrt(mean_squared_error(y_te_c, pred_vr))\n",
    "\n",
    "        # Problem.f\n",
    "        problem_check = Problem(X_tr_c, y_tr_c, X_te_c, y_te_c, [clone(m) for m in phi_check])\n",
    "        pesos_arr = np.array(weights, dtype=float)\n",
    "        rmse_custom = problem_check.f(matriz_ones, vector_ones, pesos_arr)\n",
    "\n",
    "        print(f\"   RMSE VotingRegressor: {rmse_vr:.6f}\")\n",
    "        print(f\"   RMSE Problem.f:       {rmse_custom:.6f}\")\n",
    "\n",
    "        if abs(rmse_vr - rmse_custom) > tol:\n",
    "            print(\"\\n‚ùå Diferencia detectada. Desglosando predicciones individuales:\\n\")\n",
    "            preds_ind = []\n",
    "            for i in range(n_models):\n",
    "                m_v = clone(base_models_check[i])\n",
    "                m_v.fit(X_tr_c, y_tr_c)\n",
    "                preds_ind.append(m_v.predict(X_te_c))\n",
    "            preds_ind = np.array(preds_ind)\n",
    "\n",
    "            model_preds_problem = []\n",
    "            for i in range(n_models):\n",
    "                m_p = clone(phi_check[i])\n",
    "                m_p.fit(X_tr_c, y_tr_c)\n",
    "                model_preds_problem.append(m_p.predict(X_te_c))\n",
    "            model_preds_problem = np.array(model_preds_problem)\n",
    "\n",
    "            for i in range(n_models):\n",
    "                diff = np.max(np.abs(preds_ind[i] - model_preds_problem[i]))\n",
    "                print(f\"   Modelo {i}: max abs pred diff: {diff:.6e}\")\n",
    "\n",
    "            weights_norm = np.array(weights, dtype=float) / (np.sum(weights) if np.sum(weights) != 0 else 1.0)\n",
    "            agg_vr_manual = np.average(preds_ind, axis=0, weights=weights)\n",
    "            agg_problem_manual = np.average(model_preds_problem, axis=0, weights=weights_norm)\n",
    "            agg_diff = np.max(np.abs(agg_vr_manual - agg_problem_manual))\n",
    "            print(f\"\\n   Max abs diff agregados (VR vs Problem): {agg_diff:.6e}\")\n",
    "            print(f\"   RMSE agg (VR_manual): {np.sqrt(mean_squared_error(y_te_c, agg_vr_manual)):.6f}\")\n",
    "            print(f\"   RMSE agg (Problem_manual): {np.sqrt(mean_squared_error(y_te_c, agg_problem_manual)):.6f}\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ Coinciden dentro de la tolerancia especificada.\")\n",
    "\n",
    "    # 1) Prueba con pesos uniformes (debe coincidir)\n",
    "    compare_with_weights([1.0] * n_models)\n",
    "\n",
    "    # 2) Prueba con pesos no uniformes (diagn√≥stico)\n",
    "    compare_with_weights([0.2, 0.5, 0.3][:n_models])  # adaptado al n√∫mero real de modelos\n",
    "\n",
    "    print('\\n‚úÖ SANITY CHECK completado.')"
   ],
   "id": "9878483541aa21bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Correlaci√≥n entre los modelos elegidos",
   "id": "299ba1d85b438cee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üìä DIVERSIDAD: Verificando correlaci√≥n de predicciones\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "preds_dict = {}\n",
    "rmse_scores = {}\n",
    "\n",
    "print(f\"Entrenando modelos de '{name_test}' en split de prueba...\")\n",
    "\n",
    "for i, m_template in enumerate(base_models_check):\n",
    "    m = clone(m_template)\n",
    "    m.fit(X_tr_c, y_tr_c)\n",
    "    pred = m.predict(X_te_c)\n",
    "    label = model_names_check[i] if i < len(model_names_check) else f\"M{i}\"\n",
    "    preds_dict[label]  = pred\n",
    "    rmse_scores[label] = np.sqrt(mean_squared_error(y_te_c, pred))\n",
    "\n",
    "df_preds    = pd.DataFrame(preds_dict)\n",
    "corr_matrix = df_preds.corr()\n",
    "\n",
    "print(\"\\n--- 1. Rendimiento Individual (RMSE) ---\")\n",
    "for label, rmse in rmse_scores.items():\n",
    "    print(f\"   {label}: {rmse:.4f}\")\n",
    "\n",
    "print(\"\\n--- 2. Matriz de Correlaci√≥n (Pearson) ---\")\n",
    "print(corr_matrix)\n",
    "\n",
    "mask = np.ones(corr_matrix.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "mean_corr = corr_matrix.values[mask].mean()\n",
    "print(f\"\\nüîπ Correlaci√≥n Promedio del Pool: {mean_corr:.4f}\")\n",
    "\n",
    "if mean_corr > 0.96:\n",
    "    print(\"‚ö†Ô∏è ALERTA: Correlaci√≥n MUY ALTA (>0.96). Los modelos son casi id√©nticos.\")\n",
    "elif mean_corr < 0.80:\n",
    "    print(\"‚úÖ EXCELENTE: Correlaci√≥n baja (<0.80). El ensemble funcionar√° muy bien.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è NORMAL: Correlaci√≥n moderada (0.80 - 0.96).\")\n",
    "    print(\"   (En regresi√≥n es normal que sea alta porque todos intentan predecir el mismo 'y')\")"
   ],
   "id": "48f99d74e6ce58bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Detector de Complementariedad",
   "id": "2a099a8734c1670b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def analizar_complementariedad(y_true, preds_dict):\n",
    "    \"\"\"\n",
    "    Versi√≥n mejorada: Calcula el Or√°culo y genera gr√°ficos de TODOS los pares de modelos.\n",
    "    \"\"\"\n",
    "    model_names = list(preds_dict.keys())\n",
    "    n_models = len(model_names)\n",
    "\n",
    "    # --- 1. C√ÅLCULO NUM√âRICO (Igual que antes) ---\n",
    "    squared_errors = np.zeros((len(y_true), n_models))\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üîç AN√ÅLISIS DE COMPLEMENTARIEDAD (OR√ÅCULO)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, name in enumerate(model_names):\n",
    "        pred = preds_dict[name]\n",
    "        squared_errors[:, i] = (y_true - pred) ** 2\n",
    "        rmse = np.sqrt(np.mean(squared_errors[:, i]))\n",
    "        print(f\"   RMSE {name}: {rmse:.4f}\")\n",
    "\n",
    "    # Or√°culo\n",
    "    min_errors = np.min(squared_errors, axis=1)\n",
    "    oracle_rmse = np.sqrt(np.mean(min_errors))\n",
    "    best_single_rmse = np.min(np.sqrt(np.mean(squared_errors, axis=0)))\n",
    "    mejora_potencial = 100 * (1 - oracle_rmse / best_single_rmse)\n",
    "\n",
    "    print(f\"\\nüîÆ RMSE OR√ÅCULO: {oracle_rmse:.4f}\")\n",
    "    print(f\"üöÄ Margen de Mejora: {mejora_potencial:.2f}%\")\n",
    "\n",
    "    if mejora_potencial < 5.0:\n",
    "        print(\"‚ö†Ô∏è CONCLUSI√ìN: Poca complementariedad.\")\n",
    "    else:\n",
    "        print(\"‚úÖ CONCLUSI√ìN: Alta complementariedad.\")\n",
    "\n",
    "    # --- 2. VISUALIZACI√ìN DIN√ÅMICA (TODOS CONTRA TODOS) ---\n",
    "    if n_models >= 2:\n",
    "        # Generar todas las combinaciones posibles de pares (ej. A-B, A-C, B-C)\n",
    "        pairs = list(itertools.combinations(model_names, 2))\n",
    "        n_plots = len(pairs)\n",
    "\n",
    "        # Crear figura con subgr√°ficos\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5))\n",
    "        if n_plots == 1: axes = [axes] # Parche por si solo hay 2 modelos\n",
    "\n",
    "        for ax, (m1, m2) in zip(axes, pairs):\n",
    "            # Calcular residuos (errores con signo)\n",
    "            err1 = y_true - preds_dict[m1]\n",
    "            err2 = y_true - preds_dict[m2]\n",
    "\n",
    "            # Scatter plot\n",
    "            ax.scatter(err1, err2, alpha=0.4, s=15, c='blue', edgecolors='none')\n",
    "\n",
    "            # L√≠neas de referencia (cero)\n",
    "            ax.axhline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "            ax.axvline(0, color='black', linestyle='-', linewidth=0.8)\n",
    "\n",
    "            # L√≠nea de \"Fallo Id√©ntico\" (Diagonal roja)\n",
    "            # Si los puntos est√°n en esta l√≠nea, los modelos fallan igual (malo)\n",
    "            lims = [\n",
    "                np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "                np.max([ax.get_xlim(), ax.get_ylim()])\n",
    "            ]\n",
    "            ax.plot(lims, lims, 'r--', alpha=0.8, label=\"Correlaci√≥n Total\")\n",
    "\n",
    "            # Est√©tica\n",
    "            ax.set_xlabel(f\"Residuos {m1}\")\n",
    "            ax.set_ylabel(f\"Residuos {m2}\")\n",
    "            ax.set_title(f\"{m1} vs {m2}\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# --- EJECUCI√ìN ---\n",
    "analizar_complementariedad(y_te_c, preds_dict)"
   ],
   "id": "c3daddc07bed6eef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Ejecuci√≥n del algoritmo evolutivo",
   "id": "38149f26be9ecc5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import entropy\n",
    "\n",
    "best_runs_per_dataset = {}\n",
    "for name, path in files.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîµ PROCESANDO: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è Archivo no encontrado: {path}\")\n",
    "        continue\n",
    "\n",
    "    # --- CARGAR MODELOS AJUSTADOS PARA ESTE DATASET ---\n",
    "    ruta_modelos = os.path.join(MODELOS_DIR, f\"{name}_best_models.pkl\")\n",
    "    if not os.path.exists(ruta_modelos):\n",
    "        print(f\"‚ö†Ô∏è No se encontraron modelos ajustados para '{name}' en {ruta_modelos}\")\n",
    "        print(f\"   Ejecuta primero 00_1_ajuste_modelos.ipynb y vuelve a intentarlo.\")\n",
    "        continue\n",
    "    modelos_dict = joblib.load(ruta_modelos)\n",
    "    if isinstance(modelos_dict, dict) and \"modelos\" in modelos_dict:\n",
    "        modelos_base = modelos_dict[\"modelos\"]\n",
    "        model_names = list(modelos_dict[\"nombres\"])\n",
    "    else:\n",
    "        modelos_base = modelos_dict\n",
    "        model_names = [f\"M{i}\" for i in range(len(modelos_base))]\n",
    "    print(f\"   Modelos cargados: {[type(m).__name__ for m in modelos_base]}\")\n",
    "\n",
    "    X, y = LoadData(path)\n",
    "    if X is None: continue\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "\n",
    "    dataset_results = []\n",
    "    min_rmse_dataset = float('inf')\n",
    "    best_run_data = None\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    print(f\"   Iniciando 10-Fold CV...\")\n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Clonar los modelos ajustados de este dataset para el EA\n",
    "        Phi = [clone(m) for m in modelos_base]\n",
    "\n",
    "        # --- A. EVALUAR CADA MODELO BASE POR SEPARADO ---\n",
    "        fold_baselines = {}\n",
    "        for m_name, m_template in zip(model_names, modelos_base):\n",
    "            try:\n",
    "                m_single = clone(m_template)\n",
    "                m_single.fit(X_train, y_train)\n",
    "                pred_single = m_single.predict(X_test)\n",
    "                rmse_single = np.sqrt(mean_squared_error(y_test, pred_single))\n",
    "                fold_baselines[f\"RMSE_{m_name}\"] = rmse_single\n",
    "            except Exception:\n",
    "                fold_baselines[f\"RMSE_{m_name}\"] = float('inf')\n",
    "\n",
    "        local_best_base = min(fold_baselines.values())\n",
    "\n",
    "        # --- B. EJECUTAR EA ---\n",
    "        start_t = time.time()\n",
    "        best_outputs, best_f, best_solution = EA(\n",
    "            G, N, p_c, p_m,\n",
    "            X_train, y_train,\n",
    "            Phi,\n",
    "            random_state=42\n",
    "        )\n",
    "        elapsed = time.time() - start_t\n",
    "\n",
    "        # --- C. EVALUAR EN TEST ---\n",
    "        problem_test = Problem(X_train, y_train, X_test, y_test, Phi)\n",
    "        test_rmse_ea = problem_test.f(best_solution.matriz, best_solution.vector, best_solution.pesos)\n",
    "\n",
    "        # Usar los nombres correctos para imprimir resultados\n",
    "        print_str = f\"   F{fold_idx}: EA={test_rmse_ea:.4f}\"\n",
    "        for m_name in model_names:\n",
    "            val = fold_baselines.get(f'RMSE_{m_name}', 99)\n",
    "            print_str += f\" | {m_name}={val:.4f}\"\n",
    "        print(print_str)\n",
    "\n",
    "        if test_rmse_ea < min_rmse_dataset:\n",
    "            min_rmse_dataset = test_rmse_ea\n",
    "            best_run_data = {\n",
    "                'fold':         fold_idx,\n",
    "                'solution':     best_solution,\n",
    "                'outputs':      best_outputs,\n",
    "                'f_val':        best_f,\n",
    "                'modelos_base': modelos_base,\n",
    "                'model_names':  model_names,\n",
    "                'X_train': X_train, 'y_train': y_train,\n",
    "                'X_test':  X_test,  'y_test':  y_test\n",
    "            }\n",
    "\n",
    "        row_data = {\n",
    "            'Dataset':        name,\n",
    "            'Fold':           fold_idx,\n",
    "            'RMSE_EA_Test':   test_rmse_ea,\n",
    "            'RMSE_EA_Val':    best_f,\n",
    "            'RMSE_Best_Base': local_best_base,\n",
    "            'N_Modelos':      np.sum(best_solution.vector),\n",
    "            'N_Features':     np.sum(best_solution.matriz),\n",
    "            'Time_s':         elapsed,\n",
    "        }\n",
    "        # Agregar pesos din√°micamente (crudos como los optimiza el EA)\n",
    "        for i, m_name in enumerate(model_names):\n",
    "            row_data[f'W_{m_name}'] = float(best_solution.pesos[i])\n",
    "\n",
    "        # Calcular entrop√≠a sobre pesos normalizados de modelos activos\n",
    "        pesos_raw = np.array(best_solution.pesos, dtype=float)\n",
    "        mask_active = np.array(best_solution.vector, dtype=bool)\n",
    "        pesos_activos = pesos_raw[mask_active]\n",
    "        suma_activa = pesos_activos.sum()\n",
    "        if suma_activa > 0:\n",
    "            pesos_norm = pesos_activos / suma_activa\n",
    "            row_data['W_entropy'] = float(entropy(pesos_norm + 1e-12))\n",
    "        else:\n",
    "            row_data['W_entropy'] = 0.0\n",
    "\n",
    "        row_data.update(fold_baselines)\n",
    "        dataset_results.append(row_data)\n",
    "\n",
    "    # --- FIN BUCLE FOLDS ---\n",
    "    df_temp = pd.DataFrame(dataset_results)\n",
    "    means   = df_temp.mean(numeric_only=True)\n",
    "\n",
    "    # Usar los nombres correctos para la comparativa justa\n",
    "    mean_bases = [means.get(f'RMSE_{m}', float('inf')) for m in model_names]\n",
    "    fair_best_base = min(mean_bases)\n",
    "\n",
    "    summary_row = {\n",
    "        'Dataset': 'MEDIA', 'Fold': '-',\n",
    "        'RMSE_EA_Test':   means['RMSE_EA_Test'],\n",
    "        'RMSE_EA_Val':    means['RMSE_EA_Val'],\n",
    "        'RMSE_Best_Base': fair_best_base,\n",
    "        'N_Modelos':      means['N_Modelos'],\n",
    "        'N_Features':     means['N_Features'],\n",
    "        'Time_s':         means['Time_s'],\n",
    "    }\n",
    "    for m, val in zip(model_names, mean_bases):\n",
    "        summary_row[f'RMSE_{m}'] = val\n",
    "    for i, m_name in enumerate(model_names):\n",
    "        summary_row[f'W_{m_name}'] = df_temp[f'W_{m_name}'].mean()\n",
    "    summary_row['W_entropy'] = df_temp['W_entropy'].mean()\n",
    "    df_final = pd.concat([df_temp, pd.DataFrame([summary_row])], ignore_index=True)\n",
    "\n",
    "    print(f\"\\n   ‚öñÔ∏è COMPARATIVA JUSTA ({name}):\")\n",
    "    for m, val in zip(model_names, mean_bases):\n",
    "        print(f\"      {m} (Avg): {val:.4f}\")\n",
    "    print(f\"      -----------------------------\")\n",
    "    print(f\"      üèÜ Mejor Base Global: {fair_best_base:.4f}\")\n",
    "    print(f\"      ü§ñ Tu Sistema (EA):   {means['RMSE_EA_Test']:.4f}\")\n",
    "\n",
    "    if means['RMSE_EA_Test'] < fair_best_base:\n",
    "        impr = 100 * (1 - means['RMSE_EA_Test'] / fair_best_base)\n",
    "        print(f\"      ‚úÖ RESULTADO: El EA gana por un {impr:.2f}%\")\n",
    "    else:\n",
    "        diff = 100 * (means['RMSE_EA_Test'] / fair_best_base - 1)\n",
    "        print(f\"      ‚ùå RESULTADO: El EA pierde por un {diff:.2f}% contra el especialista\")\n",
    "\n",
    "    csv_path = os.path.join(RESULTS_DIR, f\"{name}_results.csv\")\n",
    "    df_final.to_csv(csv_path, index=False)\n",
    "    print(f\"   üíæ Guardado: {csv_path}\")\n",
    "\n",
    "    if best_run_data:\n",
    "        best_runs_per_dataset[name] = best_run_data\n",
    "\n",
    "print(f\"\\nüöÄ ¬°Ejecuci√≥n completa! Revisa la carpeta {RESULTS_DIR}\")"
   ],
   "id": "4374a8767bd40f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imprimir los algoritmos de aprendizaje seleccionados, los atributos seleccionados para cada algoritmo de aprendizaje y el fitness (rmse en un conjunto de validaci√≥n interno)",
   "id": "9afc17b8fd28ceb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def PrintSolution(I, Phi):\n",
    "    Phi_index = np.where(I.vector == 1)[0]\n",
    "    for index in Phi_index:\n",
    "        attributes_selected = [l for l in range(I.matriz.shape[0]) if I.matriz[l,index] == 1]\n",
    "        print(f\"   ü§ñ Modelo: {Phi[index]}\")\n",
    "        print(f\"      Peso: {I.pesos[index]:.4f}\")  # WEIGHTED\n",
    "        print(f\"      Atributos: {attributes_selected}\")"
   ],
   "id": "b86d6066f8f84e07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualizaci√≥n de la evoluci√≥n del algoritmo",
   "id": "edbe280f25d771bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def VisualizaEvolucion(best_outputs, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.set_title(f\"Evoluci√≥n - {title}\")\n",
    "    plt.plot(best_outputs)\n",
    "    plt.xlabel(\"Generaci√≥n\")\n",
    "    plt.ylabel(\"Fitness (RMSE Val)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ],
   "id": "cf1b9a38b0577fc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Resultados en el conjunto de test",
   "id": "9ea8f1f426bea26c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "for name, data in best_runs_per_dataset.items():\n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"üìä REPORTE VISUAL PARA: {name.upper()}\")\n",
    "    print(f\"   (Mejor Fold encontrado: {data['fold']})\")\n",
    "    print(f\"{'#'*80}\\n\")\n",
    "\n",
    "    best_solution = data['solution']\n",
    "    best_outputs  = data['outputs']\n",
    "    best_f        = data['f_val']\n",
    "    modelos_base  = data['modelos_base']  # modelos ajustados espec√≠ficos del dataset\n",
    "    model_names   = data.get('model_names', [f\"M{i}\" for i in range(len(modelos_base))])\n",
    "    X_train, y_train = data['X_train'], data['y_train']\n",
    "    X_test,  y_test  = data['X_test'],  data['y_test']\n",
    "\n",
    "    print(\"--- üß¨ Estructura del Mejor Individuo (Ensemble) ---\")\n",
    "    PrintSolution(best_solution, model_names)\n",
    "    print(f\"\\nüìâ RMSE Validaci√≥n Interna (Fitness): {best_f:.4f}\")\n",
    "\n",
    "    VisualizaEvolucion(best_outputs, title=f\"{name} (Fold {data['fold']})\")\n",
    "\n",
    "    # Clonar los modelos ajustados de este dataset para evaluar en test\n",
    "    Phi_test     = [clone(m) for m in modelos_base]\n",
    "    problem_test = Problem(X_train, y_train, X_test, y_test, Phi_test)\n",
    "    rmse_test    = problem_test.f(best_solution.matriz, best_solution.vector, best_solution.pesos)\n",
    "\n",
    "    print(f\"üèÅ RMSE TEST FINAL ({name}): {rmse_test:.4f}\")\n",
    "    print(\"-\" * 80)\n"
   ],
   "id": "411c57a5988b4d00",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
