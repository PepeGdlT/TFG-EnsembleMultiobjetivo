import arff
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
import warnings

# Sklearn Imports
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.impute import SimpleImputer

# Modelos
from sklearn.ensemble import HistGradientBoostingRegressor, ExtraTreesRegressor, RandomForestRegressor, VotingRegressor
from sklearn.linear_model import Ridge, Lasso, HuberRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.svm import LinearSVR

# Ignorar warnings molestos de convergencia para mantener limpia la salida
warnings.filterwarnings("ignore")


# =============================================================================
# 1. FUNCI√ìN DE CARGA DE DATOS (INTEGRADA)
# =============================================================================
def LoadData(file_path):
    print(f"üìÇ Leyendo archivo: {file_path} ...")

    # 1. Cargar con liac-arff
    try:
        with open(file_path, 'r') as f:
            dataset = arff.load(f)
    except FileNotFoundError:
        print(f"‚ùå ERROR: No se encuentra el archivo en {file_path}")
        return None, None

    col_names = [attr[0] for attr in dataset['attributes']]
    df = pd.DataFrame(dataset['data'], columns=col_names)

    # 2. Limpieza b√°sica
    df.replace([None], np.nan, inplace=True)
    filename = file_path.lower()

    # --- CORRECCIONES ESPEC√çFICAS ---
    # CASO US CRIME: Eliminar ID in√∫til
    if 'crime' in filename and 'communityname' in df.columns:
        df = df.drop(columns=['communityname'])

    # CASO BOSTON: Arreglar columnas que cargan mal
    if 'boston' in filename:
        cols_to_fix = ['CHAS', 'RAD']
        for col in cols_to_fix:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

    # 3. FORZADO NUM√âRICO (SOLO REGRESI√ìN)
    for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # 4. BORRAR COLUMNAS VAC√çAS
    # Si una columna es 100% NaN (como 'Sex' en Abalone al forzar num√©rico), la borramos.
    df.dropna(axis=1, how='all', inplace=True)

    # 5. Imputaci√≥n de nulos restantes
    if df.isnull().sum().sum() > 0:
        print(f"   ‚ö†Ô∏è Imputando {df.isnull().sum().sum()} valores nulos con la media...")
        imputer = SimpleImputer(strategy='mean')
        data_imputed = imputer.fit_transform(df)
        df = pd.DataFrame(data_imputed, columns=df.columns)

    # 6. Separar X e y
    X = df.iloc[:, :-1].values
    y = df.iloc[:, -1].values

    print(f"   ‚úÖ Datos cargados. X: {X.shape}, y: {y.shape}")
    return X, y


# =============================================================================
# 2. CONFIGURACI√ìN DEL ZOO DE MODELOS (PERFIL "LIGHTWEIGHT" TFG)
# =============================================================================
def get_model_pool(random_state=42):
    """
    Devuelve un diccionario con modelos configurados exactamente como en el EA.
    Objetivo: Velocidad, baja varianza y generalizaci√≥n (evitar overfitting).
    """
    return {
        # --- 1. √Årboles y Ensembles (Configuraci√≥n R√°pida) ---

        # HGB: Tu configuraci√≥n optimizada para convergencia r√°pida
        'HGB': HistGradientBoostingRegressor(
            max_iter=50,  # Bajado de 100
            max_depth=3,  # √Årboles cortos
            early_stopping=True,
            scoring='loss',  # C√°lculo r√°pido
            n_iter_no_change=5,  # Paciencia baja
            random_state=random_state
        ),

        # ExtraTrees: M√°s aleatoriedad, hojas grandes para suavizar ruido
        'ExtraTrees': ExtraTreesRegressor(
            n_estimators=50,  # Menos √°rboles
            min_samples_leaf=20,  # Hojas grandes = menos nodos = m√°s r√°pido
            n_jobs=-1,  # Usa todos los n√∫cleos en este an√°lisis
            random_state=random_state
        ),

        # RandomForest: Misma l√≥gica que ET para comparar peras con peras
        'RandomForest': RandomForestRegressor(
            n_estimators=50,
            min_samples_leaf=20,
            n_jobs=-1,
            random_state=random_state
        ),

        # DT Simple: Un solo √°rbol d√©bil (base de referencia)
        'DT-Simple': DecisionTreeRegressor(
            max_depth=5,
            random_state=random_state
        ),

        # --- 2. Lineales (Estabilidad / "El Ancla") ---

        # Ridge: El complemento perfecto para los √°rboles
        'Ridge': make_pipeline(StandardScaler(), Ridge(alpha=1.0)),

        # Lasso: Selecci√≥n de caracter√≠sticas impl√≠cita
        'Lasso': make_pipeline(StandardScaler(), Lasso(alpha=0.1)),

        # Huber: Robusto a outliers (√∫til en drift abrupto con ruido)
        'Huber': make_pipeline(StandardScaler(), HuberRegressor(max_iter=100)),

        # --- 3. Instancias / Geom√©tricos (Especialistas Locales) ---

        # KNN: Captura relaciones locales que los √°rboles globales pierden
        'KNN-5': make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=5)),

        # MLP: Red neuronal muy peque√±a (similar a una regresi√≥n log√≠stica vitaminada)
        'MLP': make_pipeline(
            StandardScaler(),
            MLPRegressor(hidden_layer_sizes=(30,), max_iter=200, random_state=random_state)
        )
    }

# =============================================================================
# 3. MOTOR DE AN√ÅLISIS
# =============================================================================
def ejecutar_analisis_completo(file_path):
    # A. Carga
    X, y = LoadData(file_path)
    if X is None: return

    # B. Split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # C. Entrenamiento Individual
    model_pool = get_model_pool()
    preds_dict = {}
    errors_dict = {}  # Residuos con signo (y - pred) para correlaci√≥n
    sq_errors_dict = {}  # Errores cuadr√°ticos para RMSE y Or√°culo

    results_individual = []

    print("\nüöÄ Entrenando Zoo de Modelos...")
    for name, model in model_pool.items():
        try:
            model.fit(X_train, y_train)
            pred = model.predict(X_test)
            preds_dict[name] = pred

            # M√©tricas
            residuo = y_test - pred
            errors_dict[name] = residuo
            sq_errors_dict[name] = residuo ** 2

            rmse = np.sqrt(np.mean(residuo ** 2))
            results_individual.append({'Model': name, 'RMSE': rmse})
            print(f"   -> {name}: {rmse:.4f}")

        except Exception as e:
            print(f"   ‚ùå Fallo en {name}: {e}")

    # Mejor modelo individual (Baseline)
    df_ind = pd.DataFrame(results_individual).sort_values('RMSE')
    best_single_rmse = df_ind.iloc[0]['RMSE']
    best_single_name = df_ind.iloc[0]['Model']
    print(f"\nüèÜ Mejor Individual: {best_single_name} (RMSE: {best_single_rmse:.4f})")

    # --- D. AN√ÅLISIS DE CORRELACI√ìN (NUEVO) ---
    print("\nüî• Generando Matriz de Correlaci√≥n de Residuos...")
    matriz_corr = plot_correlation_matrix(preds_dict, y_test, title=file_path.split('/')[-1])

    # (Opcional) Imprimir parejas con menor correlaci√≥n (Mayor diversidad)
    print("   Parejas m√°s diversas (Menor correlaci√≥n):")
    corrs = matriz_corr.unstack().sort_values()
    # Filtramos para quitar autocorrelaciones (1.0) y duplicados
    unique_corrs = corrs[(corrs < 0.99) & (corrs.index.get_level_values(0) < corrs.index.get_level_values(1))]
    print(unique_corrs.head(5).to_string())

    # ---------------------------------------------------------
    # E. VOTING REGRESSOR (Benchmark Real)
    # ---------------------------------------------------------
    print("\n‚öñÔ∏è Evaluando Voting Regressor (Ensemble est√°tico de todos)...")
    voting_clf = VotingRegressor([
        (name, model) for name, model in model_pool.items() if name in preds_dict
    ])
    voting_clf.fit(X_train, y_train)
    voting_pred = voting_clf.predict(X_test)
    voting_rmse = np.sqrt(mean_squared_error(y_test, voting_pred))
    print(f"   -> Voting Regressor RMSE: {voting_rmse:.4f}")

    # ---------------------------------------------------------
    # F. OR√ÅCULO COMBINATORIO (Mejor Tr√≠o)
    # ---------------------------------------------------------
    print("\nüîç Buscando el Mejor Tr√≠o (Or√°culo)...")
    trios = list(itertools.combinations(preds_dict.keys(), 3))
    trio_results = []

    for trio in trios:
        m1, m2, m3 = trio

        # Matriz de errores cuadr√°ticos del tr√≠o (N_muestras x 3)
        trio_sq_errors = np.column_stack((
            sq_errors_dict[m1],
            sq_errors_dict[m2],
            sq_errors_dict[m3]
        ))

        # OR√ÅCULO: Para cada instancia, tomamos el error del MEJOR de los 3
        min_sq_errors = np.min(trio_sq_errors, axis=1)
        oracle_rmse = np.sqrt(np.mean(min_sq_errors))

        # Mejora porcentual sobre el mejor individual
        improvement = 100 * (1 - oracle_rmse / best_single_rmse)

        trio_results.append({
            'Trio': f"{m1} + {m2} + {m3}",
            'Oracle_RMSE': oracle_rmse,
            'Mejora_%': improvement,
            'Members': trio
        })

    df_trio = pd.DataFrame(trio_results).sort_values('Oracle_RMSE')
    best_trio = df_trio.iloc[0]

    print("\nü•á TOP 3 MEJORES TR√çOS (Complementariedad):")
    print(df_trio.head(3).to_string(index=False))

    # ---------------------------------------------------------
    # G. VISUALIZACI√ìN DEL MEJOR TR√çO
    # ---------------------------------------------------------
    m1, m2, m3 = best_trio['Members']
    err1 = errors_dict[m1]
    err2 = errors_dict[m2]
    err3 = errors_dict[m3]

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # M1 vs M2
    axes[0].scatter(err1, err2, alpha=0.4, s=15, c='blue')
    axes[0].set_xlabel(f"Residuos {m1}")
    axes[0].set_ylabel(f"Residuos {m2}")
    axes[0].set_title(f"{m1} vs {m2}")
    axes[0].grid(True, alpha=0.3)

    # M1 vs M3
    axes[1].scatter(err1, err3, alpha=0.4, s=15, c='green')
    axes[1].set_xlabel(f"Residuos {m1}")
    axes[1].set_ylabel(f"Residuos {m3}")
    axes[1].set_title(f"{m1} vs {m3}")
    axes[1].grid(True, alpha=0.3)

    # M2 vs M3
    axes[2].scatter(err2, err3, alpha=0.4, s=15, c='orange')
    axes[2].set_xlabel(f"Residuos {m2}")
    axes[2].set_ylabel(f"Residuos {m3}")
    axes[2].set_title(f"{m2} vs {m3}")
    axes[2].grid(True, alpha=0.3)

    plt.suptitle(f"An√°lisis del Ganador: {best_trio['Trio']} (Mejora Te√≥rica: {best_trio['Mejora_%']:.2f}%)",
                 fontsize=16)
    plt.tight_layout()
    plt.show()  #

    print("\n‚úÖ AN√ÅLISIS FINALIZADO.")


def plot_correlation_matrix(preds_dict, y_test, title="Matriz de Correlaci√≥n de Residuos"):
    """
    Calcula y graf√≠ca la correlaci√≥n de Pearson entre los ERRORES de los modelos.
    IMPORTANTE: No correlacionamos las predicciones, sino los residuos (y - y_pred).
    Si dos modelos fallan en los mismos puntos, tienen alta correlaci√≥n positiva (Malo).
    """
    import seaborn as sns

    # 1. Calcular Residuos (Errores con signo)
    # Si y_true es 100 y pred es 90, residuo = 10.
    # Si y_true es 100 y pred es 110, residuo = -10.
    residuals_dict = {name: y_test - pred for name, pred in preds_dict.items()}
    df_residuals = pd.DataFrame(residuals_dict)

    # 2. Calcular Matriz de Correlaci√≥n
    corr_matrix = df_residuals.corr(method='pearson')

    # 3. Graficar Heatmap
    plt.figure(figsize=(10, 8))

    # M√°scara para ocultar la mitad superior (opcional, queda m√°s limpio)
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))

    sns.heatmap(
        corr_matrix,
        mask=mask,
        annot=True,  # Mostrar n√∫meros
        fmt=".2f",  # 2 decimales
        cmap='coolwarm_r',  # Rojo=Alto(Malo), Azul=Bajo(Bueno) -> Invertido '_r' para que rojo sea alerta
        vmin=-1, vmax=1,  # Escala fija
        center=0,
        square=True,
        linewidths=.5,
        cbar_kws={"shrink": .5}
    )

    plt.title(f"Diversidad del Ensemble: {title}", fontsize=14)
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

    return corr_matrix


# =============================================================================
# ZONA DE EJECUCI√ìN (MODIFICA AQU√ç LA RUTA)
# =============================================================================
if __name__ == "__main__":
    DATASET_PATH = "../data/regression/elevators.arff"

    # Ejecutar
    ejecutar_analisis_completo(DATASET_PATH)